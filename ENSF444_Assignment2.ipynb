{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 2: Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='White'>In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.</font>\n",
        "\n",
        "You can use the Table of Content on the left side of this notebook to efficiently navigate within this documents.\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|           **Part 2: Regression**           |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    0.5    |\n",
        "| Step 3: Implement Machine Learning   Model |     1     |\n",
        "|            Step 4: Validate Mode           |     1     |\n",
        "|          Step 5: Visualize Results         |     1     |\n",
        "|                  Questions                 |     2     |\n",
        "|             Process Description            |     4     |\n",
        "|  **Part 3:   Observations/Interpretation** |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **30**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **4**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification (14.5 marks total)**\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|                  **Total**                 |  **14.5** |\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "source": [
        "!pip install yellowbrick"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXdBxb8K-noh",
        "outputId": "64e4ad0a-ab70-4ed7-fce3-fb068ddee1dc"
      },
      "id": "qXdBxb8K-noh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.10/dist-packages (1.5)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.25.2)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (0.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71bb0a9-640e-4d6d-98d9-4744437d924b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size:  262200\n",
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "4595    0\n",
            "4596    0\n",
            "4597    0\n",
            "4598    0\n",
            "4599    0\n",
            "Name: is_spam, Length: 4600, dtype: int64\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>, Type of Y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "spamData = load_spam()\n",
        "\n",
        "# TO DO: Print size and type of X and y\n",
        "x, y = spamData\n",
        "print(\"Size: \", x.size)\n",
        "print(y)\n",
        "print(f\"Type of X: {type(x)}, Type of Y: {type(y)}\")\n",
        "#X represents all of the details of the shape\n",
        "#Y represents whether or not it is spam mail (1 being yes it is, 0 being no it is not)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "X_df = pd.DataFrame(x)\n",
        "y_df = pd.DataFrame(y)\n",
        "\n",
        "#filling mode with the most frequent value\n",
        "X_df.fillna(X_df.mode().iloc[0], inplace=True)\n",
        "y_df.fillna(y_df.mode().iloc[0], inplace=True)\n",
        "\n",
        "x = X_df.values\n",
        "y = y_df.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6a3b11-b90f-4983-b3db-3cf122b63809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Create X_small and y_small\n",
        "X_small = x[:int(0.05 * len(x))]\n",
        "y_small = y[:int(0.05 * len(y))]\n",
        "print(y_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "print(\"look here\", y_train)\n",
        "\n",
        "\n",
        "spamMLmodel = LogisticRegression(max_iter=2000)\n",
        "spamMLmodel.fit(X_train, y_train)\n",
        "y_predUNO = spamMLmodel.predict(X_test)\n",
        "\n",
        "spamMLmodel = LogisticRegression(max_iter=2000)\n",
        "\n",
        "spamMLmodel.fit(X_train[:, :2], y_train[:, :2])\n",
        "y_predDUOS = spamMLmodel.predict(X_test[:, :2])\n",
        "\n",
        "\n",
        "spamMLmodel = LogisticRegression(max_iter=2000)\n",
        "spamMLmodel.fit(X_train[:int(0.05 * len(X_train))],  y_train[:int(0.05 * len(y_train))])\n",
        "y_predTRES = spamMLmodel.predict(X_test[:int(0.05 * len(X_test))])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTEHoTUNF9lU",
        "outputId": "7475b3f6-4347-418a-9172-fa0634db9850"
      },
      "id": "BTEHoTUNF9lU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "look here [[0]\n",
            " [1]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "spamMLmodel.fit(X_train, y_train)\n",
        "trainOne = accuracy_score(y_train, spamMLmodel.predict(X_train))\n",
        "valdOne = accuracy_score(y_test, y_predUNO)\n",
        "\n",
        "newOne = LogisticRegression(max_iter=2000)\n",
        "newOne.fit(X_train[:, :2], y_train[:, :2])\n",
        "trainTwo = accuracy_score(y_train[:, :2], newOne.predict(X_train[:, :2]))\n",
        "valdTwo = accuracy_score(y_test[:, :2], y_predDUOS)\n",
        "\n",
        "\n",
        "spamMLmodel.fit(X_train[:int(0.05 * len(X_train))],  y_train[:int(0.05 * len(y_train))])\n",
        "trainThree = accuracy_score(y_train[:int(0.05 * len(y_train))], spamMLmodel.predict(X_train[:int(0.05 * len(y_train))]))\n",
        "valdThree = accuracy_score(y_test[:int(0.05 * len(y_test))], y_predTRES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ouRzZRZL1O-",
        "outputId": "a980b2c1-4835-41ee-9dd1-e1d92cc4be13"
      },
      "id": "3ouRzZRZL1O-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks for steps 3-5)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c147b0c-ec84-4807-bf18-18d6670a89bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Data Size  Training Accuracy  Validation Accuracy\n",
            "0     3680.0           0.933696             0.908696\n",
            "1     3680.0           0.620109             0.589130\n",
            "2      184.0           0.945652             0.913043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-6f2e37b5b02b>:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Data Size': data_size, 'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy}, ignore_index=True)\n",
            "<ipython-input-18-6f2e37b5b02b>:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Data Size': data_size, 'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy}, ignore_index=True)\n",
            "<ipython-input-18-6f2e37b5b02b>:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append({'Data Size': data_size, 'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy}, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame(columns=['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
        "values = [trainOne, valdOne, trainTwo, valdTwo, trainThree, valdThree]\n",
        "sizesRef = [y_train.shape[0], 0, y_train[:, :2].shape[0], 0, y_train[:int(0.05 * len(y_train))].shape[0], 0]\n",
        "\n",
        "for i in range(0, len(values),2):\n",
        "    if i < len(values):\n",
        "      data_size = sizesRef[i]\n",
        "      train_accuracy = values[i]\n",
        "      val_accuracy = values[i + 1]\n",
        "      results = results.append({'Data Size': data_size, 'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy}, ignore_index=True)\n",
        "\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Questions (4 marks)**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "<b>\n",
        "1.\n",
        "The more information we have on each sample, the more accurate the training and validation set becomes. This is seen through the second row as this is when we only use two categories and the training and validation accuracy is much lower compared to the other two that used fifty two categories. This shows the significance of having different features. While the first row in the table uses more data as it uses all of the information compared to the last one that only uses 5% of the data, it has a slightly higher accuracy. This could be due to a combination of the model's ability to generalize and the specific characteristics of the sampled data indcating that the model is robust and can generalize effectively even with less data, or that the sampled subset is representative of the overall data distribution. <br> This comparison emphasizes the significance of feature selection and the impact it can have on model performance.\n",
        "\n",
        "<br>\n",
        "\n",
        "2. False Negative -  Says it is spam when it isnt<br>\n",
        "False Positive - Says it is not a spam when it is<br>  \n",
        "The worst in this case is false positives, as spam can be potentially dangerous or harmful to the computer or the user itself. It is ok if a couple that are not spam to be classified as such as the user, if waiting for an email or just casual wants can go to the spam box and look through it with catious. This prevents the probability of that harm.\n",
        "\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "<b>\n",
        "\n",
        "1. The code is on this ipynb file<br>\n",
        "\n",
        "2. I went step by step, as I thought this was the most effective as they all relied on each other  <br>\n",
        "\n",
        "3. I used the one built in collab AI to aid me in basic syntax or how to find the size of the tuple etc. For collab's AI I would prompt it with whatever question I had wihtout any specific format. I used chatGPT to help me understand any bugs I was experiencing. In chat i'd past the code and state before \"explain the error in the code but dont give me the code solution\". I did not need to modify the code the AI gave me as I did not use it for logic in the code. <br>\n",
        "\n",
        "4. Honestly, it was really hard to know what to search for especially for questions 3-5. To solve it I read through the slides one more time in mind of the coding questions and was able to derive stuff for 3 and 4. While as fr four I had to do my own research and ask AI for a form of guidance. <br>\n",
        "I also had a challenge of deciding how ot fill up the na valuese as I didnt know which one was best for the data.\n",
        " I used SKLearns API website and Educative.io to understand what each function was meant to do. Other than that I would just google \"what is the accuracy formula in sklearn\" or \"how to fill na values\", and from there i'd do my research and retrive the code for it from the respective website.\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression (10.5 marks total)**\n",
        "\n",
        "| **Question**                               | **Point** |\n",
        "|--------------------------------------------|-----------|\n",
        "| **Part 2: Regression**                     |           |\n",
        "| Step 1: Data Input                         | 1         |\n",
        "| Step 2: Data Processing                    | 0.5       |\n",
        "| Step 3: Implement Machine Learning   Model | 1         |\n",
        "| Step 4: Validate Mode                      | 1         |\n",
        "| Step 5: Visualize Results                  | 1         |\n",
        "| Questions                                  | 2         |\n",
        "| Process Description                        | 4         |\n",
        "| **Total**                                  | **10.5**  |\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c63021c-ed62-4d47-d13c-1a6ddce58b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size, 8240\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>, Type of Y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import concrete dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "concreteData = load_concrete()\n",
        "\n",
        "x,y = concreteData\n",
        "\n",
        "# TO DO: Print size and type of X and y\n",
        "print(\"Size,\", x.size)\n",
        "print(f\"Type of X: {type(x)}, Type of Y: {type(y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "X_df = pd.DataFrame(x)\n",
        "y_df = pd.DataFrame(y)\n",
        "\n",
        "#filling mode with the most frequent value\n",
        "X_df.fillna(X_df.mode().iloc[0], inplace=True)\n",
        "y_df.fillna(y_df.mode().iloc[0], inplace=True)\n",
        "\n",
        "x = X_df.values\n",
        "y = y_df.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945",
        "outputId": "633f1214-6039-4391-cc2f-0455b6fa975d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4f5dd6a9136f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Note: for any random state parameters, you can use random_state = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_train = model.predict(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "#training validation\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "\n",
        "#accuracy validation\n",
        "mse_val = mean_squared_error(y_test, y_pred)\n",
        "r2_val = r2_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdb737e-3f39-452b-9395-674a8ba94086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Training Accuracy Validation Accuracy\n",
            "MSE             110.345501           95.635335\n",
            "R2 Score          0.609071            0.636898\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame(index=['MSE', 'R2 Score'], columns=['Training Accuracy', 'Validation Accuracy'])\n",
        "\n",
        "results.loc['MSE', 'Training Accuracy'] = mse_train\n",
        "results.loc['R2 Score', 'Training Accuracy'] = r2_train\n",
        "results.loc['MSE', 'Validation Accuracy'] = mse_val\n",
        "results.loc['R2 Score', 'Validation Accuracy'] = r2_val\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Questions (2 marks)**\n",
        "1. <b> Did using a linear model produce good results for this dataset? Why or why not? </b> <br>\n",
        "The linear regression model performs reasonably well on the concrete dataset.\n",
        "R2 Score ranges from 0 to 1, with 1 being the best. Both the training and validation R2 scores are decent (0.609071 for training and 0.636898 for validation). This suggests that the model explains a reasonable amount of variance in the target variable, but there might still be room for improvement. The lower the MSE, the better. Both the training and validation MSE values are not extremely low, indicating that there might be room for improvement. However, the validation MSE is slightly lower than the training MSE, which is a positive sign, suggesting that the model generalizes well to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>Explain YOUR PROCESS here:</b></font>\n",
        "\n",
        "1. My code can be found in the underneath each step in this ipynb file.\n",
        "2.I went step by step because I thought that would be the most effective as most of the steps relied on one another.  \n",
        "3. For this the only generative AI prompt I used was what is the equation for R2 and MSE in ChatGPT.\n",
        "4. For this one I didnt have any struggles or challenges. This is because the first part is very similar to this so I took that to this part. Also since we learnt about MSE and R2 in class it was easy to come up with the analysis of the numbers in the results dataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR FINDINGS HERE</b></font>\n",
        "For starters we cant compare part 2 and part 1 against each other becuase they are completely different models with different factors that influence the binary classification.\n",
        "\n",
        "So lets start with part 1,\n",
        "-  There is a noticeable difference in accuracy scores across different subsets.\n",
        "For example, the first row of the results DataFrame shows a data size of 3680.0, with relatively high training (0.933696) and validation (0.908696) accuracy.\n",
        "- The second row, also with a data size of 3680.0, has lower training (0.620109) and validation (0.589130) accuracy.\n",
        "- The third row, with a smaller data size (184.0), has high training (0.945652) and validation (0.913043) accuracy.\n",
        "\n",
        "part 2,\n",
        "- The model has a Training R2 Score of 0.609071, indicating that about 60.91% of the variance in the target variable is explained by the model on the training data.\n",
        "- The Validation R2 Score of 0.636898 suggests that the model's performance is comparable on the validation set, indicating reasonable generalization.\n",
        "\n",
        "In lectures, we often discuss the importance of understanding the characteristics of the dataset, such as its size, types of features, and target variables. This information is crucial for preprocessing and selecting appropriate machine learning algorithms.\n",
        "\n",
        "\n",
        "Additionally, evaluating model performance using metrics like accuracy helps in assessing how well the model generalizes to new, unseen data. It is important to analyze both in R2 and MSE and the accuracy equation to see to what extent is this model accurate or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "# **Part 4: Reflection (2 marks)**\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR THOUGHTS HERE</b></font>\n",
        "<br>\n",
        "Liked: I liked how engaging this assignment is.  \n",
        "Disliked: Would have been nice to go over these codes in the lab or something as that would save a lot of time and errors in searching for the respective code.\n",
        "<br> <br> <br>\n",
        "Interesting: How fast everything happened and how accurate the results become just based off of one function. Im sure before the skitlearn library there would have been many lines of code to come up with to get the result but now i can call one library. <br>\n",
        "Confusing: The steps honestly, as I have never seen code on how to analyze models in python it was hard to know what needed to be done and how it can be done. <br>\n",
        "Challenging: When errors came out it was hard to solve it as it was hard to visualize the data. If I were to visualize it accurate I would be able to solve the errors much quicker. <br>\n",
        "Motivating: Especially in part one, being able to work on a problem that is so relevant to my own life made the learning process much more engaging and motivating."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 5: Bonus Question (4 marks)**\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd68a162-7e84-4378-b98f-287e75fc0cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.636936690685576\n",
            "0.6386805871822323\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=100)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "y_pred_train_ridge = ridge_model.predict(X_train)\n",
        "\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(r2_ridge)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=2)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "y_pred_train_lasso = lasso_model.predict(X_train)\n",
        "\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "print(r2_lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>ANSWER </b></font> <br>\n",
        "r2_ridge, r2_lasso COMPARING TO 0.636898<br>\n",
        "alpha = 1.0 <br>\n",
        "0.6368985034309533\n",
        "0.6380351738941974\n",
        "<br>\n",
        "alpha = 0.001<br>\n",
        "0.636898110734286\n",
        "0.6368994935284487\n",
        "<br>\n",
        "alpha = 50<br>\n",
        "0.6369175830698732\n",
        "0.615819732153774\n",
        "\n",
        "\n",
        "<br>\n",
        "The lasso regression with an alpha of 1, produces a slightly larger value (0.6380351738941974) which is a better score for the accuracy of the data, compared to other alphas that we tested. However ridge regression has a very miniscual change in the thousandths place, which dosent provide much of an advantage, this can be seen with the tests of alpha when its 0.001, 1.0, and 50.\n",
        "<br> <br> Is this good enough? Its almost the same as using the regular linear regression as both in ridge and lasso it will still result in 64% of varaince in the target which is good but not the best.  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
